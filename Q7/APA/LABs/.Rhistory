# fem un cop d'ull primer a les dades de VA
plot(valid.sample$input, valid.sample$target)
set.seed (7) # per igualar els resultats de tothom (esperem!)
n <- 20
set.seed(222)
set.seed(222)
library(MASS)
N <- 200
(Sigma <- matrix(data=c(2,1.3,1.3,1),nrow=2,ncol=2,byrow=TRUE))
eigen (Sigma, only.values=TRUE)$values
mean.1 <- matrix(c(2,0),nrow=2,ncol=1)
X.red <- mvrnorm(N,mu=mean.1,Sigma=Sigma)
mean.2 <- -mean.1
X.green <- mvrnorm(N,mu=mean.2,Sigma=Sigma)
par(mfrow=c(2,2))
plot(c(X.red[,1],X.green[,1]), c(X.red[,2],X.green[,2]),
xlim=c(-6,6),ylim=c(-6,6),
col=c(rep('red',N),rep('green',N)), main="Toy data", xlab="X1", ylab="X2")
d <- data.frame(c(rep(1,N),rep(2,N)), c(X.red[,1], X.green[,1]), c(X.red[,2], X.green[,2]))
colnames(d) <- c("target", "X1", "X2")
d$target <- as.factor(d$target)
summary(d)
# call to FDA (also known as LDA, because there is a strong relationship with it, see a later lecture)
myLDA <- lda(d[c(2,3)],d[,1])
LDAslope <- myLDA$scaling[2]/myLDA$scaling[1]
plot(c(X.red[,1],X.green[,1]), c(X.red[,2],X.green[,2]), col=c(rep('red',N),rep('green',N)),
xlim=c(-6,6),ylim=c(-6,6),
main="Direction for projection using FDA", xlab="X1", ylab="X2")
abline(0,LDAslope,col='black',lwd=2)
myLDA.proj <- d[,2] * myLDA$scaling[1] + d[,3] * myLDA$scaling[2]
plot(myLDA.proj, c(rep(0,N),rep(0,N)), col=c(rep('green',N),rep('red',N)),
main='FDA projection as seen in 1D', xlab="Discriminant", ylab="")
myLDA
myLDA$scaling
myPCA <- prcomp(d[c(2,3)],scale=TRUE)
d1PCA <- myPCA$x[,1]
PCAslope1 <- myPCA$rotation[2,1]/myPCA$rotation[1,1]
plot(c(X.red[,1],X.green[,1]), c(X.red[,2],X.green[,2]), col=c(rep('red',N),rep('green',N)),
xlim=c(-6,6),ylim=c(-6,6),
main="Direction for projection using PCA", xlab="X1", ylab="X2")
abline(0,PCAslope1,col='black',lwd=2)
## the crabs data is in the MASS package
data(crabs)
## look at data
?crabs
summary(crabs)
head(crabs)
Crabs.class <- factor(paste(crabs[,1],crabs[,2],sep=""))
# Now 'BF' stands now for 'Blue Female', and so on
table(Crabs.class)
## using the rest of the variables as predictors (except 'index', which is only an index)
Crabs <- crabs[,4:8]
summary(Crabs)
par(mfrow=c(1,1))
boxplot(Crabs)
hist(Crabs$FL,col='red',breaks=20,xlab="", main='Frontal Lobe Size (mm)')
hist(Crabs$RW,col='red',breaks=20,xlab="", main='Rear Width (mm)')
hist(Crabs$CL,col='red',breaks=20,xlab="", main='Carapace Length (mm)')
hist(Crabs$CW,col='red',breaks=20,xlab="", main='Carapace Width (mm)')
hist(Crabs$BD,col='red',breaks=20,xlab="", main='Body Depth (mm)')
(lda.model <- lda (x=Crabs, grouping=Crabs.class))
plot(lda.model)
Leptograpsus Crabs
Leptograpsus Crabs
loadings <- as.matrix(Crabs) %*% as.matrix(lda.model$scaling)
colors.crabs <- c('blue', 'lightblue', 'orange', 'yellow')
crabs.plot <- function (myloadings)
{
plot (myloadings[,1], myloadings[,2], type="n", xlab="LD1", ylab="LD2")
text (myloadings[,1], myloadings[,2], labels=crabs$index, col=colors.crabs[unclass(Crabs.class)], cex=.55)
legend('topright', c("B-M","B-F","O-M","O-F"), fill=colors.crabs, cex=.55)
}
crabs.plot (loadings)
Crabs.new <- data.frame (New.feature = loadings, Target = Crabs.class)
summary(Crabs.new)
lda.model
lda.model$scaling
library(rgl)
# 3D scatterplot (can be rotated and zoomed with the mouse)
plot3d(loadings[,1], loadings[,2], loadings[,3], "LD1", "LD2", "LD3",
size = 4, col=colors.crabs[unclass(Crabs.class)], main="Crabs Data")
text3d(loadings[,1], loadings[,2], loadings[,3], color = "black", texts=rownames(Crabs.new), adj = c(0.85, 0.85), cex=0.6)
(lda.logmodel <- lda (x=log(Crabs), grouping=Crabs.class))
logloadings <- as.matrix(log(Crabs)) %*% as.matrix(lda.logmodel$scaling)
crabs.plot (logloadings)
set.seed(222)
# the cclust library contains some clustering functions, including k-means
library (cclust)
install.packages("cclust")
n1 <- 30
n2 <- 40
n3 <- 50
# create cluster 1
x1 <- rnorm (n1,1,0.5)
y1 <- rnorm (n1,1,0.5)
# create cluster 2
x2 <- rnorm (n2,2,0.5)
y2 <- rnorm (n2,6,0.7)
# create cluster 3
x3 <- rnorm (n3,7,1)
y3 <- rnorm (n3,7,1)
# create the data
x <- rbind (cbind(x1,y1), cbind(x2,y2), cbind(x3,y3))
c <- c(rep("4", n1), rep("2", n2), rep("3", n3))
D <- data.frame (x,color=c)
## this is your data
plot(D$x1,D$y1)
## and these are the true clusters
plot(D$x1,D$y1,col=as.vector(D$color))
k <- 3 # yeah, this is tricky, why 3?
## execute k-means with a maximum of 100 iterations
kmeans.3 <- cclust (x,k,iter.max=100,method="kmeans",dist="euclidean")
## plot initial and final prototypes (cluster centers)
points(kmeans.3$initcenters, pch=19)
points(kmeans.3$centers, pch=19)
## draw arrows to see the process
arrows (kmeans.3$initcenters[,1], kmeans.3$initcenters[,2], kmeans.3$centers[,1], kmeans.3$centers[,2])
## plot and paint the clusters (according to the computed assignments)
plot(D$x1,D$y1,col=(kmeans.3$cluster+1))
## plot the cluster centers
points(kmeans.3$centers,col=seq(1:kmeans.3$ncenters)+1,cex=2,pch=19)
(CH.3 <- clustIndex(kmeans.3,x, index="calinski"))
k <- 5 # guess what is going to happen?
## this is your data
plot(D$x1,D$y1)
## and these are the true clusters
plot(D$x1,D$y1,col=as.vector(D$color))
k <- 3 # yeah, this is tricky, why 3?
## execute k-means with a maximum of 100 iterations
kmeans.3 <- cclust (x,k,iter.max=100,method="kmeans",dist="euclidean")
## plot initial and final prototypes (cluster centers)
points(kmeans.3$initcenters, pch=19)
points(kmeans.3$centers, pch=19)
## draw arrows to see the process
arrows (kmeans.3$initcenters[,1], kmeans.3$initcenters[,2], kmeans.3$centers[,1], kmeans.3$centers[,2])
## plot and paint the clusters (according to the computed assignments)
plot(D$x1,D$y1,col=(kmeans.3$cluster+1))
## execute k-means with a maximum of 100 iterations
kmeans.3 <- cclust (x,k,iter.max=100,method="kmeans",dist="euclidean")
install.packages("cclust")
## execute k-means with a maximum of 100 iterations
kmeans.3 <- cclust (x,k,iter.max=100,method="kmeans",dist="euclidean")
install.packages("cclust")
set.seed(222)
# the cclust library contains some clustering functions, including k-means
library (cclust)
n1 <- 30
n2 <- 40
n3 <- 50
# create cluster 1
x1 <- rnorm (n1,1,0.5)
y1 <- rnorm (n1,1,0.5)
# create cluster 2
x2 <- rnorm (n2,2,0.5)
y2 <- rnorm (n2,6,0.7)
# create cluster 3
x3 <- rnorm (n3,7,1)
y3 <- rnorm (n3,7,1)
# create the data
x <- rbind (cbind(x1,y1), cbind(x2,y2), cbind(x3,y3))
c <- c(rep("4", n1), rep("2", n2), rep("3", n3))
D <- data.frame (x,color=c)
## this is your data
plot(D$x1,D$y1)
## and these are the true clusters
plot(D$x1,D$y1,col=as.vector(D$color))
k <- 3 # yeah, this is tricky, why 3?
## execute k-means with a maximum of 100 iterations
kmeans.3 <- cclust (x,k,iter.max=100,method="kmeans",dist="euclidean")
## plot initial and final prototypes (cluster centers)
points(kmeans.3$initcenters, pch=19)
points(kmeans.3$centers, pch=19)
## draw arrows to see the process
arrows (kmeans.3$initcenters[,1], kmeans.3$initcenters[,2], kmeans.3$centers[,1], kmeans.3$centers[,2])
## plot and paint the clusters (according to the computed assignments)
plot(D$x1,D$y1,col=(kmeans.3$cluster+1))
## plot the cluster centers
points(kmeans.3$centers,col=seq(1:kmeans.3$ncenters)+1,cex=2,pch=19)
(CH.3 <- clustIndex(kmeans.3,x, index="calinski"))
k <- 5 # guess what is going to happen?
## execute k-means with a maximum of 100 iterations
kmeans.5 <- cclust (x,k,iter.max=100,method="kmeans",dist="euclidean")
## this is your data again
plot(D$x1,D$y1,col=as.vector(D$color))
## plot initial and final prototypes (centers)
points(kmeans.5$initcenters, pch=19)
points(kmeans.5$centers, pch=19)
## draw arrows to see the process
arrows (kmeans.5$initcenters[,1], kmeans.5$initcenters[,2], kmeans.5$centers[,1], kmeans.5$centers[,2])
## plot and paint the clusters (according to the computed assignments)
plot(D$x1,D$y1,col=(kmeans.5$cluster+1))
## plot the cluster centers
points(kmeans.5$centers,col=seq(1:kmeans.5$ncenters)+1,cex=2,pch=19)
(CH.5 <- clustIndex(kmeans.5,x, index="calinski"))
## the MASS library contains the multivariate Gaussian
library(MASS)
## the ggplot2 library contains functions for making nicer plots
library(ggplot2)
install.packages("ggplot2")
## the ggplot2 library contains functions for making nicer plots
library(ggplot2)
set.seed(333)
# GENERATE DATA FROM A MIXTURE OF 2D GAUSSIANS
generate.data <- function(N, K, prior.mean, prior.var)
{
p <- length(prior.mean)
# generate random mixture centres from the prior
mu_k <- mvrnorm(K, mu=prior.mean, Sigma=diag(prior.var, 2))
# generate mixture coefficients
pi_k <- runif(K)
pi_k <- pi_k/sum(pi_k)
# generate the data
obs <- matrix(0, nrow=N, ncol=p)
z <- numeric(N)
sigma_k <- matrix(0, nrow=K, ncol=p)
for (i in 1:K)
sigma_k[i,] <- runif(p)
for (i in 1:N)
{
# draw the observation from a component according to coefficient
z[i] <- sample(1:K, 1, prob=pi_k)
# draw the observation from the corresponding mixture location
obs[i,] <- mvrnorm(1, mu=mu_k[z[i],], Sigma=diag(sigma_k[z[i],],p))
}
list(locs=mu_k, z=z, obs=obs, coefs=pi_k)
}
# plot 2D data from a mixture
plot.mixture <- function(locs, z, obs)
{
stopifnot(dim(obs)[2]==2)
z <- as.factor(z)
df1 <- data.frame(x=obs[,1], y=obs[,2], z=z)
df2 <- data.frame(x=locs[,1], y=locs[,2])
p <- ggplot()
p <- p + geom_point(data=df1, aes(x=x, y=y, colour=z), shape=16, size=2, alpha=0.75)
p <- p + geom_point(data=df2, aes(x=x, y=y), shape=16, size=3)
p <- p + theme(legend.position="none")
p
}
# plot 2D data as a scatter plot
plot.data <- function(dat)
{
stopifnot(dim(dat)[2]==2)
df1 <- data.frame(x=dat[,1], y=dat[,2])
p <- ggplot()
p <- p + geom_point(data=df1, aes(x=x, y=y), size=2, alpha=0.75)
p
}
n <- 1000
k <- 5
centre <- c(0,0)
dispersion <- 10
## generate 2D data as a mixture of 5 Gaussians, each axis-aligned (therefore the two variables are independent) with different variances; the centers and coefficients of the mixture are chosen randomly
d <- generate.data (n,k,centre,dispersion)
## these are the components of the mixture
plot.mixture(d$locs, d$z, d$obs)
## compute 2D kernel density
z <- kde2d(d$obs[,1], d$obs[,2], n=50)
## some pretty colors
library(RColorBrewer)
colorets <- rev(brewer.pal(11, "RdYlBu"))
## this is the raw data (what the clustering method "sees")
plot(d$obs, xlab="x", ylab="y", pch=19, cex=.4)
## and this is a contour plot of the unconditional density
contour(z, drawlabels=FALSE, nlevels=22, col=colorets, add=TRUE)
abline(h=mean(d$obs[,2]), v=mean(d$obs[,1]), lwd=2)
## a simpler way of plotting the data
plot.data(d$obs)
k <- 2
kmeans2.2 <- cclust (d$obs,k,iter.max=100,method="kmeans",dist="euclidean")
plot(d$obs[,1],d$obs[,2],col=(kmeans2.2$cluster+1))
points(kmeans2.2$centers,col=seq(1:kmeans2.2$ncenters)+1,cex=2,pch=19)
(CH2.2 <- clustIndex(kmeans2.2,d$obs, index="calinski"))
k <- 5
kmeans2.5 <- cclust (d$obs,k,iter.max=100,method="kmeans",dist="euclidean")
plot(d$obs[,1],d$obs[,2],col=(kmeans2.5$cluster+1))
points(kmeans2.5$centers,col=seq(1:kmeans2.5$ncenters)+1,cex=2,pch=19)
(CH2.5 <- clustIndex(kmeans2.5,d$obs, index="calinski"))
do.kmeans <- function (what.k)
{
r <- cclust (d$obs,what.k,iter.max=100,method="kmeans",dist="euclidean")
(clustIndex(r,d$obs, index="calinski"))
}
max (r <- replicate (100, do.kmeans(5)))
# This may take a while for large datasets
res <- vector("numeric", 10)
for (k in 2:10)
res[k] <- max (r <- replicate (100, do.kmeans(k)))
res[1] <- NA
plot(res, type="l", axes = FALSE, xlab="k", ylab="Calinski-Harabasz")
axis(side = 1, at = 2:10)
axis(side = 2, at = seq(0,5000,500))
grid(9, 6, lwd = 2)
library(Rmixmod)
install.packages("Rcpp")
library(Rmixmod)
install.packages("Rmixmod")
library(Rmixmod)
fammodel <- mixmodGaussianModel (family="diagonal", equal.proportions=FALSE)
z <- mixmodCluster (data.frame(d$obs), models = fammodel, nbCluster = 5)
summary(z)
# the final centers
(means <- z@bestResult@parameters@mean)
# if you want "hard" assignments
(found.clusters <- z@bestResult@partition)
## the estimated covariance matrices for each cluster
(z@bestResult@parameters@variance)
## self-explained
(z@bestResult@likelihood)
## the posterior probabilities or "soft" assignments (the gamma_c(x_i) in class)
z@bestResult@proba
## Let's have a look at the first 5 data points:
z@bestResult@proba[1:5,]
plot(d$obs[,1],d$obs[,2],col=(found.clusters+1))
points(means,col="black",cex=2,pch=19)
fammodel <- mixmodGaussianModel (family="general", equal.proportions=FALSE)
z <- mixmodCluster (data.frame(d$obs),models = fammodel, nbCluster = 5)
summary(z)
means <- z@bestResult@parameters@mean
fammodel <- mixmodGaussianModel (family="diagonal", equal.proportions=FALSE)
z <- mixmodCluster (data.frame(d$obs), models = fammodel, nbCluster = 5)
summary(z)
# the final centers
(means <- z@bestResult@parameters@mean)
# if you want "hard" assignments
(found.clusters <- z@bestResult@partition)
## the estimated covariance matrices for each cluster
(z@bestResult@parameters@variance)
## self-explained
(z@bestResult@likelihood)
## the posterior probabilities or "soft" assignments (the gamma_c(x_i) in class)
z@bestResult@proba
## Let's have a look at the first 5 data points:
z@bestResult@proba[1:5,]
plot(d$obs[,1],d$obs[,2],col=(found.clusters+1))
points(means,col="black",cex=2,pch=19)
fammodel <- mixmodGaussianModel (family="diagonal", equal.proportions=FALSE)
z <- mixmodCluster (data.frame(d$obs), models = fammodel, nbCluster = 5)
summary(z)
# the final centers
(means <- z@bestResult@parameters@mean)
# if you want "hard" assignments
(found.clusters <- z@bestResult@partition)
## the estimated covariance matrices for each cluster
(z@bestResult@parameters@variance)
## self-explained
(z@bestResult@likelihood)
## the posterior probabilities or "soft" assignments (the gamma_c(x_i) in class)
z@bestResult@proba
## Let's have a look at the first 5 data points:
z@bestResult@proba[1:5,]
plot(d$obs[,1],d$obs[,2],col=(found.clusters+1))
points(means,col="black",cex=2,pch=19)
fammodel <- mixmodGaussianModel (family="general", equal.proportions=FALSE)
z <- mixmodCluster (data.frame(d$obs),models = fammodel, nbCluster = 5)
summary(z)
means <- z@bestResult@parameters@mean
found.clusters <- z@bestResult@partition
plot(d$obs[,1],d$obs[,2],col=(found.clusters+1))
points(means,col="black",cex=2,pch=19)
## compare the estimated centers
means
# with the truth (note the clusters may appear in a different order)
d$locs
## or the estimated coefficients
sort(z@bestResult@parameters@proportions)
# with the truth
sort(d$coefs)
