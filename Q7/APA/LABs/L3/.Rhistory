set.seed(222)
library(MASS)
N <- 200
(Sigma <- matrix(data=c(2,1.3,1.3,1),nrow=2,ncol=2,byrow=TRUE))
eigen (Sigma, only.values=TRUE)$values
mean.1 <- matrix(c(2,0),nrow=2,ncol=1)
X.red <- mvrnorm(N,mu=mean.1,Sigma=Sigma)
mean.2 <- -mean.1
X.green <- mvrnorm(N,mu=mean.2,Sigma=Sigma)
par(mfrow=c(2,2))
plot(c(X.red[,1],X.green[,1]), c(X.red[,2],X.green[,2]),
xlim=c(-6,6),ylim=c(-6,6),
col=c(rep('red',N),rep('green',N)), main="Toy data", xlab="X1", ylab="X2")
d <- data.frame(c(rep(1,N),rep(2,N)), c(X.red[,1], X.green[,1]), c(X.red[,2], X.green[,2]))
colnames(d) <- c("target", "X1", "X2")
d$target <- as.factor(d$target)
# definim 'i'
i <- sqrt(as.complex(-1))
(A <- matrix(c(0,i,0,1),nrow=2,byrow = TRUE))
# definim 'i'
i <- sqrt(as.complex(-1))
(A <- matrix(c(0,i,0,1),nrow=2,byrow = TRUE))
(A %*% t(A))  # A·A^T és la matriu (-1 i; i 1)
(t(A) %*% A)  # A^T·A és la matriu tot zeros
N <- 10
(X <- matrix(c(rep(1,N), seq(N)),nrow=N))
t <- seq(10,20,length.out=N) + rnorm(N)
plot(X[,2],t,lwd=3)
(C <- t(X) %*% X)  # X^T X
# definim 'i'
i <- sqrt(as.complex(-1))
(A <- matrix(c(0,i,0,1),nrow=2,byrow = TRUE))
(A %*% t(A))  # A·A^T és la matriu (-1 i; i 1)
(t(A) %*% A)  # A^T·A és la matriu tot zeros
N <- 10
(X <- matrix(c(rep(1,N), seq(N)),nrow=N))
t <- seq(10,20,length.out=N) + rnorm(N)
plot(X[,2],t,lwd=3)
(C <- t(X) %*% X)  # X^T X
(X.pseudo <- solve(C) %*% t(X))  # (X^T X)^{-1} X^T
(X.pseudo %*% X) # és pseudo-inversa esquerra d'X
(w <- X.pseudo %*% t) # solució del problema
lines (X[,2], w[2,1]*X[,2]+w[1,1], type="l")
(s <- svd(X))
D <- diag(s$d)
s$u %*% D %*% t(s$v)
D <- diag(1/s$d)
(w <- s$v %*% D %*% t(s$u) %*% t)
(mostra <- data.frame(x=X,t=t))
# 1. desactivar-lo (el "-1" a la fòrmula següent) i usar la nostra pròpia columna de 1's
model1 <- glm (t ~ x.2 + x.1 - 1, data=mostra, family = gaussian)
# 2. usar la que glm ja posa (recomanat) i desactivar la nostra pròpia columna de 1's
model2 <- glm (t ~ x.2, data=mostra, family = gaussian)
model1$coefficients
model2$coefficients
eps <- 1e-3
(X.eps <- matrix(c(1,eps,0,1,0,eps),nrow=3))
((C.eps <- t(X.eps) %*% X.eps))
solve(C.eps) # comencem a tenir problemes ...
eps <- 1e-10
(X.eps <- matrix(c(1,eps,0,1,0,eps),nrow=3))
(C.eps <- t(X.eps) %*% X.eps)
solve(C.eps) # dóna error (la matriu 2x2 "tot uns" és singular)
kappa(X, exact=TRUE)
kappa(t(X) %*% X, exact=TRUE)
(X <- matrix(c(rep(1,N), 100+seq(N)),nrow=N))
kappa(X, exact=TRUE)
kappa(t(X) %*% X, exact=TRUE)
(X <- matrix(c(rep(1,N), 100+seq(N)),nrow=N))
X[,2] <- X[,2] - mean(X[,2])
X
kappa(X, exact=TRUE)
kappa(t(X) %*% X, exact=TRUE)
library(MASS)
ginv(X)
bodyfat.data <- read.table(file = "bodyfatdata.txt", header=FALSE, col.names = c('triceps', 'thigh', 'midarm', 'bodyfat'))
summary(bodyfat.data)
attach(bodyfat.data)
N <- nrow(bodyfat.data)
(model <- lm(bodyfat ~ ., data = bodyfat.data))
summary(model)
dens <- density(model$residuals)
hist(model$residuals, prob=T)
lines(dens,col="red")
library(car)
qqPlot(model)
# This is how we can compute the mean square error
prediction <- predict(model)
install.packages("car")
qqPlot(model)
library(car)
qqPlot(model)
install.packages("car")
library(car)
qqPlot(model)
# This is how we can compute the mean square error
prediction <- predict(model)
(mean.square.error <- sum((bodyfat - prediction)^2)/N)
(norm.mse <- sum((bodyfat - prediction)^2) / ((N-1)*var(bodyfat)))
(R.squared <- (1 - norm.mse)*100)
plot(bodyfat, predict(model))
(LOOCV <- sum( (model$residuals/(1-ls.diag(model)$hat))^2) / N)
# and the corresponding predictive R-square
(R2.LOOCV = (1 - LOOCV*N/((N-1)*var(bodyfat)))*100)
library(MASS)
# notice we start with a wide logarithmic search
lambdas <- 10^seq(-6,2,0.1)
select(lm.ridge(bodyfat ~ triceps + thigh + midarm, lambda = lambdas))
# we perform a finer search
lambdas <- seq(0,1,0.001)
select(lm.ridge(bodyfat ~ triceps + thigh + midarm, lambda = lambdas))
(bodyfat.ridge.reg <- lm.ridge(bodyfat ~ triceps + thigh + midarm, lambda = 0.019))
X <- cbind(rep(1,length=length(bodyfat)),triceps, thigh, midarm)
(w <- ginv(X) %*% bodyfat)
model$coefficients
R2.LOOCV
(1 - bodyfat.ridge.reg$GCV)*100
library(glmnet)
t <- as.numeric(bodyfat.data[,4])
x <- as.matrix(bodyfat.data[,1:3])
model.lasso <- cv.glmnet (x, t, nfolds = length(t))
plot(model.lasso)
coef(model.lasso)
# lambda.min is the value of lambda that gives minimum mean cross-validated error
model.lasso$lambda.min
# Predictions can be made based on the fitted cv.glmnet object; for instance, this would be the TR error with the "optimal" lambda as chosen by LOOCV
predict (model.lasso, newx = x, s = "lambda.min")
# And this would be corresponding LOOCV
LOOCV <- model.lasso$cvm[model.lasso$lambda == model.lasso$lambda.min]
# and the corresponding predictive R-square
(R2.LOOCV = (1 - LOOCV*N/((N-1)*var(bodyfat)))*100)
